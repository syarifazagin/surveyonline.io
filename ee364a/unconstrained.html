
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Sample Quiz Title</title>
    <link href='http://fonts.googleapis.com/css?family=Josefin+Sans|Alike' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="quiz.css" type="text/css" />
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML.js"></script>

    <script type="text/javascript">
    $(document).ready(function(){
      //close all the content divs on page load
      $('.response').hide();

      // toggle slide
      $('.selection').click(function(){
        // by calling sibling, we can use same div for all demos
        $(this).siblings('.response').slideToggle('fast');
      });

      $('button').click(function(event){
        var $target = $(event.target);
        var $checkboxes = $target.parent('.mcq').find('input');
        for (var i = 0; i < $checkboxes.length; i++) {
          var $checkbox = $checkboxes.eq(i);
          a = $checkbox;
          if ($checkbox[0].checked && $checkbox.nextAll('.response').hasClass('right')) {
            $checkbox.nextAll('.correct-checkbox').show();
          } else if ($checkbox[0].checked &&
              $checkbox.nextAll('.response').hasClass('wrong')) {
            $checkbox.nextAll('.incorrect-checkbox').show();
          } else if (!$checkbox[0].checked &&
              $checkbox.nextAll('.response').hasClass('right')) {
            $checkbox.nextAll('.incorrect-checkbox').show();
          } else {
            $checkbox.nextAll('.correct-checkbox').show();
          }
        }
        $target.parent('.mcq').find('.response').slideToggle('fast');
        if ($target.text() == 'Submit') {
          $target.text('Hide');
        } else {
          $checkboxes.nextAll('.incorrect-checkbox').hide();
          $checkboxes.nextAll('.correct-checkbox').hide();
          $target.text('Submit');
        }
      });
    });
    </script>
  </head>

<body>
  $\newcommand{\ones}{\mathbf 1}$
  <div>
	<h1>Algorithms for unconstrained optimization</h1>
	<fieldset>
		<div>
			<div class="description">
In descent methods, the particular
choice of search direction does not matter so much.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
In descent methods, the particular
choice of line search does not matter so much.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
When the gradient descent method is started from a point near the
solution, it will converge very quickly.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
	</fieldset>
	<br/>
	<fieldset>
		<div>
			<div class="description">
Newton's method with step size $h=1$ always works.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
When Newton's method is started from a point near the
solution, it will converge very quickly.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
Using Newton's method to minimize $f(Ty)$, where $Ty=x$ and $T$ is nonsingular,
can greatly improve the convergence speed when $T$ is chosen appropriately.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
	</fieldset>
	<br/>
	<fieldset>
		<div>
			<div class="description">
If $f$ is self-concordant, its Hessian is Lipschitz continuous.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
If the Hessian of $f$ is Lipschitz continuous, then $f$ is self-concordant.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
Newton's method should only be used to minimize self-concordant functions.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
$f(x) = \exp x$ is self-concordant.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
		<hr/>
		<div>
			<div class="description">
$f(x) = -\log x$ is self-concordant.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
			</ol>
		</div>
	</fieldset>
	<br/>
	<fieldset>
		<div class="intro">Consider the problem of minimizing
\[
f(x) = (c^Tx)^4 + \sum_{i=1}^n w_i \exp x_i,
\]
over $x \in \mathbf{R}^n$, where $w \succ 0$.</div>
		<hr/>
		<div>
			<div class="description">
Newton's method would probably require fewer iterations than the gradient
method, but each iteration would be much more costly.</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">True.</div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
					</div>
				</li>
				<li class="choice">
					<div class="selection">False.</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
	</fieldset>
	<br/>
	<fieldset>
		<div>
			<div class="description">
Newton's method is seldom used in machine learning because</div>
			<ol type="a">
				<li class="choice">
					<div class="selection">common loss functions are not self-concordant </div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						 While this is true, it is
not the reason Newton's method isn't used.
					</div>
				</li>
				<li class="choice">
					<div class="selection">Newton's method does not work well on noisy data </div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						 This statement doesn't
even make sense.
					</div>
				</li>
				<li class="choice">
					<div class="selection">machine learning researchers don't really understand linear algebra </div>
					<div class="response wrong">
						<span class="wrong">Incorrect. </span>
						
It is known that at least some machine learning researchers do know linear
algebra.
					</div>
				</li>
				<li class="choice">
					<div class="selection">it is generally not practical to form or store the Hessian in such problems, due to large problem size</div>
					<div class="response right">
						<span class="right">Correct! </span>
						
					</div>
				</li>
			</ol>
		</div>
	</fieldset>
	<br/>
</div>

  <footer>
  Page generated using <a href="https://github.com/karanveerm/quizgen">Quizgen</a>
  </footer>
</body>
</html>
